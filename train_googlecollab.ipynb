{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l1J5svUDgYbV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1J5svUDgYbV",
        "outputId": "b8ac2238-153a-4932-c6b8-ac95364b9dbd"
      },
      "outputs": [],
      "source": [
        "# install vizdoom\n",
        "!pip install vizdoom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NsPTPGg8gZvM",
      "metadata": {
        "id": "NsPTPGg8gZvM"
      },
      "outputs": [],
      "source": [
        "# google collab\n",
        "import shutil\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TmeCWN0KgabN",
      "metadata": {
        "id": "TmeCWN0KgabN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad8c427",
      "metadata": {
        "id": "0ad8c427"
      },
      "outputs": [],
      "source": [
        "#misc\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "# env\n",
        "import gymnasium as gym\n",
        "from vizdoom import gymnasium_wrapper\n",
        "# learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d24882",
      "metadata": {
        "id": "87d24882"
      },
      "source": [
        "Tensorboard for viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60fc3d95",
      "metadata": {
        "id": "60fc3d95"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "SummaryWriter(log_dir=\"tensorboard_data\")\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50f8944",
      "metadata": {
        "id": "d50f8944"
      },
      "source": [
        "# Setup for DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8db23c",
      "metadata": {
        "id": "4b8db23c"
      },
      "source": [
        "check for cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4469466f",
      "metadata": {
        "id": "4469466f"
      },
      "outputs": [],
      "source": [
        "# Utilize GPU for training if GPU present\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b129a4c",
      "metadata": {
        "id": "4b129a4c"
      },
      "source": [
        "Image preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412f1908",
      "metadata": {
        "id": "412f1908"
      },
      "outputs": [],
      "source": [
        "transformer = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((45,60)),\n",
        "    # T.Grayscale(),\n",
        "    T.ToTensor()\n",
        "])\n",
        "def preprocess(obs):\n",
        "    # returns shape [1, C, H, W] where C = 1 because gray\n",
        "    # C IS 3 NOW BECAUSE NO GRAYSCALING\n",
        "    return transformer(obs[\"screen\"]).squeeze(1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6430b1d",
      "metadata": {
        "id": "a6430b1d"
      },
      "source": [
        "We first need a **Transition** class which represents a `(state, action) -> (state', reward)` datapoint.\n",
        "\n",
        "Then we need a **Replay Memory** class to store and utilize these transitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da0fa68",
      "metadata": {
        "id": "8da0fa68"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple(\n",
        "    \"Transition\",\n",
        "    [\"obs\", \"next_obs\", \"action\", \"reward\", \"done\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae744278",
      "metadata": {
        "id": "ae744278"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, buffer_size):\n",
        "        # deque's FIFO structure will forget older memories as the agent explores\n",
        "        self.memory = deque([], buffer_size)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, next_obs, action, rew, done):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        t = Transition(preprocess(obs).clone().detach(),\n",
        "                       preprocess(next_obs).clone().detach(),\n",
        "                       int(action),\n",
        "                       float(rew),\n",
        "                       bool(done)\n",
        "        )\n",
        "        self.memory.append(t)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d21e0b",
      "metadata": {
        "id": "d7d21e0b"
      },
      "source": [
        "# DQN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16564f28",
      "metadata": {
        "id": "16564f28"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_actions) -> None:\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=7),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=4),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, 45, 60)\n",
        "            x = self.conv2(self.conv1(x))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        print(flattened_size)\n",
        "\n",
        "        self.lin1 = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 800),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        self.lin2 = nn.Sequential(\n",
        "            nn.Linear(800, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1) # x.size(0) = batch_size\n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fea2781",
      "metadata": {
        "id": "6fea2781"
      },
      "source": [
        "# Define environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616b8145",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "616b8145",
        "outputId": "563e0b32-9dc9-44f0-da4e-47c27867b3f4"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"VizdoomBasic-v0\", frame_skip = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc03db81",
      "metadata": {
        "id": "dc03db81"
      },
      "source": [
        "We will add some boilerplate for the training, and some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91dee4c",
      "metadata": {
        "id": "c91dee4c"
      },
      "outputs": [],
      "source": [
        "# Initialize RNG seed\n",
        "seed:int = 42 #rng seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "total_timesteps:int = 500000 # timestep max of an experiment\n",
        "lr:float = 0.01\n",
        "buffer_size:int = 10000 # experience replay buffer size\n",
        "gamma: float = 0.99 # discount factor\n",
        "batch_size: int = 64 # batch size for experience replay buffer sampling\n",
        "epsilon_max: float = 1 # starting epsilon value (exploration/exploitation)\n",
        "epsilon_min:float = 0.1 # ending epsilon value\n",
        "epsilon_duration:float = 0.5 # time spent before min epsilon is reached\n",
        "# training_start:int = 5000 # steps needed before training begins\n",
        "tnur: int = 1 # target network update rate\n",
        "tnuf: int = 500 # target network update frequency\n",
        "qntf: int = 10 # qnetwork training frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcdf199c",
      "metadata": {
        "id": "bcdf199c"
      },
      "source": [
        "Epsilon decay let's us start by picking random actions, then slowly start picking actions that yield high rewards. We first explore a wide array of options, and once we have an idea of what works and what doesn't, we start exploiting that knowledge and ldive deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3caea6d2",
      "metadata": {
        "id": "3caea6d2"
      },
      "outputs": [],
      "source": [
        "def epsilon_decay(current_timestep: int):\n",
        "    if current_timestep < 100000: return epsilon_max\n",
        "    if current_timestep > 200000: return epsilon_min\n",
        "    return (200000 - current_timestep)/(200000-100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b479e3e2",
      "metadata": {
        "id": "b479e3e2"
      },
      "outputs": [],
      "source": [
        "def get_action(action_space:gym.Space, obs, q_net:DQN, current_timestep:int):\n",
        "    rng = random.random()\n",
        "    epsilon = epsilon_decay(current_timestep)\n",
        "    if rng > epsilon:\n",
        "        # action with highest q_value\n",
        "        q_values = q_net(obs.unsqueeze(0).to(device))\n",
        "        action = torch.argmax(q_values).cpu().numpy()\n",
        "    else:\n",
        "        # random action\n",
        "        action = action_space.sample()\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1a14f7",
      "metadata": {
        "id": "af1a14f7"
      },
      "source": [
        "Define model, optimizer, and replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4a92fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b4a92fa",
        "outputId": "58c90b7b-0716-48a6-c8fd-eef4cdde5a8c"
      },
      "outputs": [],
      "source": [
        "# Initialize agent & target network\n",
        "q_net = DQN(env.action_space.n).to(device)\n",
        "optimizer = torch.optim.Adam(q_net.parameters(), lr)\n",
        "# Target network is used to evaluate the progress of our DQN.\n",
        "# It represents the past policy from which we evaluate surplus reward gains.\n",
        "target_net = DQN(env.action_space.n).to(device)\n",
        "target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "# Initialize Experience Replay (ER) buffer\n",
        "# ER is used in DQN to avoid catastrophic forgetting.\n",
        "# It allows the model to re-train on previous experiences in order to\n",
        "# mix it with novel experiences and not forget previous training.\n",
        "# Another benefit of ER is that by randomly sampling data from memory\n",
        "# we avoid sequential correlation of experiences.\n",
        "replay_buffer = ReplayBuffer(buffer_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a1f0c8",
      "metadata": {
        "id": "b2a1f0c8"
      },
      "source": [
        "We need to be able to read the memory, get a `batch_size` amount of transitions, and make that into a batch with tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a364b8",
      "metadata": {
        "id": "64a364b8"
      },
      "outputs": [],
      "source": [
        "def collate(batch) -> dict:\n",
        "    batch_dict = {}\n",
        "    batch_dict[\"obs\"] = torch.stack([t.obs for t in batch]).to(device) # shape [B, C, H, W]\n",
        "    batch_dict[\"next_obs\"] = torch.stack([t.next_obs for t in batch]).to(device) # shape [B, C, H, W]\n",
        "    batch_dict[\"action\"] = torch.tensor([t.action for t in batch]).to(device) # shape [B]\n",
        "    batch_dict[\"reward\"] = torch.tensor([t.reward for t in batch]).to(device) # shape [B]\n",
        "    batch_dict[\"done\"] = torch.tensor([t.done for t in batch]).to(device) # shape [B]\n",
        "    return batch_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98693f9",
      "metadata": {
        "id": "e98693f9"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4364b7b",
      "metadata": {
        "id": "a4364b7b"
      },
      "outputs": [],
      "source": [
        "def train(episode_losses, global_step):\n",
        "    # create batch from memory\n",
        "    batch = collate(replay_buffer.sample(batch_size))\n",
        "    # get predictions in the form of q-values over all actions\n",
        "    q_values = q_net(batch[\"obs\"])\n",
        "    # get the prediction of the action that was actually taken\n",
        "    q_values_for_actions = q_values.gather(1, batch[\"action\"].unsqueeze(1)).squeeze(1)\n",
        "    # get the prediction of the target network\n",
        "    # target_net_max = target_net(batch[\"next_obs\"]).max(dim=1)[0]\n",
        "    # target q-values based on bellman equation\n",
        "    q_target = batch[\"reward\"] + gamma * q_net(batch[\"next_obs\"]).max(dim=1)[0] * (1 - batch[\"done\"].float())\n",
        "    # BACKPROP\n",
        "    # compute loss\n",
        "    loss = nn.functional.mse_loss(q_values_for_actions, q_target)\n",
        "    writer.add_scalar(\"Training Loss\", loss, global_step)\n",
        "    episode_losses.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0feb45e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0feb45e8",
        "outputId": "9449d305-936a-4405-abf8-010c5e8b3346"
      },
      "outputs": [],
      "source": [
        "obs, info = env.reset(seed=42)\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "current_episode_reward = 0\n",
        "for global_step in tqdm(range(total_timesteps)):\n",
        "    # get action epsilon-greedy\n",
        "    action = get_action(env.action_space, preprocess(obs), q_net, global_step)\n",
        "    # step through the env\n",
        "    next_obs, rew, term, trun, info = env.step(action)\n",
        "    writer.add_scalar(\"Training Reward\", rew, global_step)\n",
        "    current_episode_reward += rew\n",
        "    # store transition into memory\n",
        "    replay_buffer.push(obs, next_obs, action, rew, term or trun) # term or trun = done\n",
        "    # update obs\n",
        "    obs = next_obs\n",
        "    # env reset when finished\n",
        "    if term or trun:\n",
        "        obs, info = env.reset()\n",
        "        episode_rewards.append(current_episode_reward)\n",
        "        current_episode_reward = 0\n",
        "\n",
        "    # TRAINING\n",
        "    if global_step > batch_size:\n",
        "       train(episode_losses, global_step)\n",
        "\n",
        "    # periodically save model\n",
        "    if global_step % 50000 == 0:\n",
        "        torch.save(q_net.state_dict(), f\"checkpoints/dqn_step_{global_step}.pth\")  # works in Colab\n",
        "        # OR: move to CPU before saving to ensure portability\n",
        "        torch.save(q_net.cpu().state_dict(), f\"checkpoints/dqn_cpu_step_{global_step}.pth\")\n",
        "        q_net.to(device)  # optionally move back to GPU after saving\n",
        "    # make sure that all pending events have been written to disk\n",
        "    writer.flush()\n",
        "\n",
        "env.close()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U5_sPpMtgsx-",
      "metadata": {
        "id": "U5_sPpMtgsx-"
      },
      "outputs": [],
      "source": [
        "folder_path = \"checkpoints\"\n",
        "\n",
        "# Create a zip file from the folder\n",
        "shutil.make_archive(\"checkpoints\", \"zip\", folder_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(\"checkpoints.zip\")\n",
        "\n",
        "# Zip the logs\n",
        "shutil.make_archive(\"tensorboard_logs\", \"zip\", \"runs\")\n",
        "\n",
        "# Download to your local machine\n",
        "files.download(\"tensorboard_logs.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b89f4f",
      "metadata": {
        "id": "99b89f4f"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6372fccd",
      "metadata": {
        "id": "6372fccd"
      },
      "outputs": [],
      "source": [
        "env:gym.Env = gym.make(\"VizdoomBasic-v0\")\n",
        "# Re-create the model architecture\n",
        "model = DQN(env.action_space.n)\n",
        "model.load_state_dict(torch.load(\"dqn.pth\"))\n",
        "model.eval()  # Set to eval mode if you're not training\n",
        "\n",
        "obs, info = env.reset(seed=42)\n",
        "for _ in range(500):\n",
        "    # get action\n",
        "    q_values = q_net(preprocess(obs).unsqueeze(0).to(device))\n",
        "    action = torch.argmax(q_values).cpu().numpy()\n",
        "    # step\n",
        "    obs, rew, term, trun, info = env.step(action)\n",
        "\n",
        "    if term or trun:\n",
        "        obs, info = env.reset()\n",
        "\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
