{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9b434",
   "metadata": {},
   "source": [
    "# google colab boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d6741b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_google_colab:bool = 'google.colab' in str(get_ipython())\n",
    "is_google_colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2196eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_google_colab:\n",
    "    # install vizdoom\n",
    "    !pip install vizdoom\n",
    "    # google collab\n",
    "    import shutil\n",
    "    from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad8c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#misc\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import deque, namedtuple\n",
    "from copy import deepcopy\n",
    "# env\n",
    "import gymnasium as gym\n",
    "from vizdoom import gymnasium_wrapper\n",
    "# learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d24882",
   "metadata": {},
   "source": [
    "Tensorboard for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fc3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f8944",
   "metadata": {},
   "source": [
    "# Setup for DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8db23c",
   "metadata": {},
   "source": [
    "check for cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4469466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize GPU for training if GPU present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b129a4c",
   "metadata": {},
   "source": [
    "Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412f1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((45,60)),\n",
    "    T.Grayscale(),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def preprocess(obs):\n",
    "    # returns shape [1, C, H, W] where C = 1 because gray\n",
    "    return transformer(obs[\"screen\"]).squeeze(1).to(device)\n",
    "\n",
    "def stack_frames(frames):\n",
    "    return torch.cat(list(frames), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6430b1d",
   "metadata": {},
   "source": [
    "We first need a **Transition** class which represents a `(state, action) -> (state', reward)` datapoint.\n",
    "\n",
    "Then we need a **Replay Memory** class to store and utilize these transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da0fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\", \n",
    "    [\"obs\", \"next_obs\", \"action\", \"reward\", \"done\"]\n",
    ")\n",
    "\n",
    "def make_transition(obs_frames, next_obs_frames, action, rew, done) -> Transition:\n",
    "    t = Transition(obs_frames.clone().detach(), \n",
    "                   next_obs_frames.clone().detach(), \n",
    "                   int(action), \n",
    "                   float(rew),\n",
    "                   bool(done)\n",
    "    )\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae744278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size):\n",
    "        # deque's FIFO structure will forget older memories as the agent explores\n",
    "        self.memory = deque([], buffer_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, t:Transition):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(t)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d21e0b",
   "metadata": {},
   "source": [
    "# DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16564f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        in_channels = 4\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 2 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea2781",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616b8145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomBasic-v0\", render_mode=\"human\")\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03db81",
   "metadata": {},
   "source": [
    "We will add some boilerplate for the training, and some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91dee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNG seed\n",
    "seed:int = 42 #rng seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "total_timesteps:int = 400000 # timestep max of an experiment\n",
    "lr:float = 0.01\n",
    "buffer_size:int = 10000 # experience replay buffer size\n",
    "gamma: float = 0.99 # discount factor\n",
    "batch_size: int = 128 # batch size for experience replay buffer sampling\n",
    "epsilon_max: float = 1 # starting epsilon value (exploration/exploitation)\n",
    "epsilon_min:float = 0.1 # ending epsilon value\n",
    "epsilon_decay_min = 100000 # total_timesteps / 4\n",
    "epsilon_decay_max = 200000 # total_timesteps / 1.7\n",
    "# epsilon_duration:float = 0.5 # time spent before min epsilon is reached\n",
    "training_start:int = 128 # steps needed before training begins\n",
    "tnuf: int = 100 # target network update frequency\n",
    "qntf: int = 4 # qnetwork training frequency (deepmind dqn baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf199c",
   "metadata": {},
   "source": [
    "Epsilon decay let's us start by picking random actions, then slowly start picking actions that yield high rewards. We first explore a wide array of options, and once we have an idea of what works and what doesn't, we start exploiting that knowledge and ldive deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3caea6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(current_timestep: int):\n",
    "    if current_timestep < epsilon_decay_min: return epsilon_max\n",
    "    if current_timestep > epsilon_decay_max: return epsilon_min\n",
    "    epsilon_decaying = (epsilon_decay_max - current_timestep)/(epsilon_decay_max-epsilon_decay_min)\n",
    "    return max(epsilon_min, epsilon_decaying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b479e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(action_space:gym.Space, obs, q_net:DQN, current_timestep:int):\n",
    "    # print(\" === GETTING ACTION === \")\n",
    "    rng = random.random()\n",
    "    epsilon = epsilon_decay(current_timestep)\n",
    "    # print(\"obs shape:\", obs.unsqueeze(0).shape)\n",
    "\n",
    "    # FOR TESTING\n",
    "    # epsilon = 0.01\n",
    "    if rng > epsilon:\n",
    "        # action with highest q_value\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(obs.unsqueeze(0).to(device))\n",
    "            action = int(torch.argmax(q_values).cpu().numpy())\n",
    "            # print(\"action picked from\", q_values, \" ->\", action, type(action))\n",
    "    else:\n",
    "        # random action \n",
    "        action = int(action_space.sample())\n",
    "        # print(\"random action ->\", action, type(action))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a14f7",
   "metadata": {},
   "source": [
    "Define q network, optimizer, and target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b4a92fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize agent & target network\n",
    "q_net = DQN(env.action_space.n).to(device)\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr)\n",
    "# Target network is used to evaluate the progress of our DQN.\n",
    "# It represents the past policy from which we evaluate surplus reward gains.\n",
    "target_net = DQN(env.action_space.n).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f5ede",
   "metadata": {},
   "source": [
    "# Initialize Experience Replay (ER) buffer\n",
    "ER is used in DQN to avoid catastrophic forgetting. It allows the model to re-train on previous experiences in order to mix it with novel experiences and not forget previous training. Another benefit of ER is that by randomly sampling data from memory we avoid sequential correlation of experiences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "558c2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1f0c8",
   "metadata": {},
   "source": [
    "We need to be able to read the memory, get a `batch_size` amount of transitions, and make that into a batch with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a364b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch) -> dict:\n",
    "    \"\"\"regarding the shapes:\n",
    "    a preprocessed frame is [1, 45, 60] (grayscale screenshot)\n",
    "    \n",
    "    we stack a few adjacent frames to make a [4, 45, 60] tensor\n",
    "\n",
    "    when we batch these stacks, the final batch will be of size [B, C, H, W] where:\n",
    "      - B (batch size) = 128\n",
    "      - C (channels) = 4 because we are stacking our 1 channel images\n",
    "      - H & W = 45,60 from the image dimensions\n",
    "    \"\"\"\n",
    "    batch_dict = {}\n",
    "    batch_dict[\"obs\"] = torch.stack([t.obs for t in batch]).to(device) # shape [B, C, H, W]\n",
    "    batch_dict[\"next_obs\"] = torch.stack([t.next_obs for t in batch]).to(device) # shape [B, C, H, W]\n",
    "    batch_dict[\"action\"] = torch.tensor([t.action for t in batch]).to(device) # shape [B]\n",
    "    batch_dict[\"reward\"] = torch.tensor([t.reward for t in batch]).to(device) # shape [B]\n",
    "    batch_dict[\"done\"] = torch.tensor([t.done for t in batch]).to(device) # shape [B]\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f472046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_frames(obs):\n",
    "    obs_frames = deque([], maxlen=4)\n",
    "    # fill the stack first\n",
    "    frame = preprocess(obs)\n",
    "    for _ in range(4):\n",
    "        obs_frames.append(frame)\n",
    "    return obs_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98693f9",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4364b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(global_step):\n",
    "    # create batch from memory\n",
    "    batch = collate(replay_buffer.sample(batch_size))\n",
    "    # get predictions in the form of q-values over all actions\n",
    "    q_values = q_net(batch[\"obs\"])\n",
    "    writer.add_scalar(\"QValue/mean\", q_values.mean(), global_step)\n",
    "    # get the prediction of the action that was actually taken\n",
    "    q_values_for_actions = q_values.gather(1, batch[\"action\"].unsqueeze(1)).squeeze(1)\n",
    "    # get the prediction of the target network\n",
    "    target_net_max = target_net(batch[\"next_obs\"]).max(dim=1)[0]\n",
    "    # target q-values based on bellman equation\n",
    "    q_target = batch[\"reward\"] + gamma * target_net_max * (1 - batch[\"done\"].float())\n",
    "    # BACKPROP\n",
    "    # compute loss\n",
    "    loss = nn.functional.mse_loss(q_target, q_values_for_actions)\n",
    "    # print(loss)\n",
    "    writer.add_scalar(\"Training Loss\", loss, global_step)\n",
    "    optimizer.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "# get frames\n",
    "obs_frames = reset_frames(obs)\n",
    "# copy over for next_obs\n",
    "next_obs_frames = deepcopy(obs_frames)\n",
    "current_episode_reward = 0\n",
    "\n",
    "for global_step in tqdm(range(total_timesteps)):\n",
    "    # get action epsilon-greedy\n",
    "    action = get_action(env.action_space, stack_frames(obs_frames), q_net, global_step)\n",
    "    # step through the env\n",
    "    # print(\"action before stepping\", 3, \"type:\", type(3))\n",
    "    next_obs, reward, term, trun, info = env.step(action)\n",
    "    current_episode_reward += reward\n",
    "    next_obs_frames.append(preprocess(next_obs))\n",
    "    # store transition into memory\n",
    "    t = make_transition(stack_frames(obs_frames), \n",
    "                        stack_frames(next_obs_frames), \n",
    "                        action, \n",
    "                        reward, \n",
    "                        term or trun) # term or trun = done\n",
    "    replay_buffer.push(t) \n",
    "    # update obs\n",
    "    obs_frames = next_obs_frames\n",
    "    # env reset when finished\n",
    "    if term or trun:\n",
    "        obs, info = env.reset()\n",
    "        # get frames\n",
    "        obs_frames = reset_frames(obs)\n",
    "        # copy over for next_obs\n",
    "        next_obs_frames = deepcopy(obs_frames)\n",
    "        writer.add_scalar(\"Episode Reward\", current_episode_reward, global_step)\n",
    "        current_episode_reward = 0\n",
    "\n",
    "    # TRAINING\n",
    "    if global_step > training_start and global_step % qntf == 0:\n",
    "        train(global_step)\n",
    "        # print(q_net.conv1[0].weight)\n",
    "    if global_step % tnuf == 0:\n",
    "        # print(target_net.fc_layers[3].weight)\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "    \n",
    "    # make sure that all pending events have been written to disk\n",
    "    writer.flush()\n",
    "\n",
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model\n",
    "torch.save(q_net.state_dict(), \"model_dqn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049dfbd",
   "metadata": {},
   "source": [
    "# google colab download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_google_colab:\n",
    "    files.download(\"model_dqn.pth\")\n",
    "    shutil.make_archive(\"runs\", \"zip\", \"runs\")\n",
    "    files.download(\"runs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455889c8",
   "metadata": {},
   "source": [
    "### frame stacking demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de71304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# env = gym.make(\"VizdoomBasic-v0\", render_mode=\"human\")\n",
    "# obs, info = env.reset(seed=42)\n",
    "\n",
    "# # get frames\n",
    "# obs_frames = deque([], maxlen=4)\n",
    "# # fill the stack first\n",
    "# for _ in range(4):\n",
    "#     frame = preprocess(obs)\n",
    "#     obs_frames.append(frame)\n",
    "# # copy over for next_obs\n",
    "# next_obs_frames = deepcopy(obs_frames)\n",
    "\n",
    "# for _ in range(100):\n",
    "#     action = env.action_space.sample()\n",
    "#     next_obs, rew, term, trun, info = env.step(action)\n",
    "#     next_obs_frames.append(preprocess(next_obs))\n",
    "\n",
    "#     t = make_transition(stack_frames(obs_frames), stack_frames(next_obs_frames), action, rew, term or trun) # term or trun = done\n",
    "#     replay_buffer.push(t)\n",
    "\n",
    "#     obs_frames = next_obs_frames\n",
    "    \n",
    "#     if term or trun:\n",
    "#         obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb6d30",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6372fccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m q_values \u001b[38;5;241m=\u001b[39m model(stack_frames(obs_frames)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmax(q_values)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 13\u001b[0m obs, rew, term, trun, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m obs_frames\u001b[38;5;241m.\u001b[39mappend(preprocess(obs))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/introDL-vizdoom/.venv/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/introDL-vizdoom/.venv/lib/python3.9/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/introDL-vizdoom/.venv/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/introDL-vizdoom/.venv/lib/python3.9/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:134\u001b[0m, in \u001b[0;36mVizdoomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall `reset` before using `step` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m env_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_env_action(action)\n\u001b[0;32m--> 134\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[1;32m    136\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mis_episode_finished()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"VizdoomBasic-v0\", frame_skip=2, render_mode=\"human\")\n",
    "# Re-create the model architecture\n",
    "model = DQN(env.action_space.n)\n",
    "model.load_state_dict(torch.load(\"models/model_dqn_highLR.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()  # Set to eval mode if you're not training\n",
    "\n",
    "obs, info = env.reset(seed=42)\n",
    "obs_frames = reset_frames(obs)\n",
    "for _ in range(10000):\n",
    "    # get action\n",
    "    q_values = model(stack_frames(obs_frames).unsqueeze(0).to(device))\n",
    "    action = int(torch.argmax(q_values).cpu().numpy())\n",
    "    obs, rew, term, trun, info = env.step(action)\n",
    "    obs_frames.append(preprocess(obs))\n",
    "    # break\n",
    "    \n",
    "    if term or trun:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
